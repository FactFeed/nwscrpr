# Bangla News Scraper - Enhanced Version

A robust, production-ready Python package for scraping news articles from Bangladeshi news websites. This enhanced version features improved architecture, caching, error handling, and comprehensive testing.

## ‚ú® Key Features

- üèóÔ∏è **Modern Architecture**: Clean, modular design with abstract base classes
- üöÄ **High Performance**: Async-ready with smart caching mechanism  
- üì∞ **Multi-Site Support**: Prothom Alo, The Daily Ittefaq (easily extensible)
- üìÑ **Complete Data Extraction**: Title, content, author, date, URL, main image
- üñºÔ∏è **Smart Image Detection**: Extracts featured/hero images automatically
- üî§ **Unicode Support**: Proper UTF-8 encoding for Bengali text
- üíæ **Multiple Output Formats**: JSON, CSV with structured data
- üõ°Ô∏è **Robust Error Handling**: Custom exceptions and retry mechanisms
- ‚è±Ô∏è **Rate Limiting**: Configurable delays to respect website policies
- üìä **Detailed Logging**: Configurable logging levels with progress tracking
- üóÑÔ∏è **Intelligent Caching**: File-based caching to avoid duplicate requests
- ‚úÖ **Data Validation**: Comprehensive validation with type safety
- üß™ **Well Tested**: Comprehensive unit test suite
- üì¶ **Easy Installation**: pip installable package with CLI tools

## üöÄ Quick Start

### Installation

```bash
# Install from source
git clone https://github.com/rayatchowdhury/BD-Newspaper-Scraper.git
cd BD-Newspaper-Scraper
pip install -e .

# Or install dependencies directly
pip install -r requirements.txt
```

### Basic Usage

```bash
# Scrape 5 articles from Prothom Alo
python main.py --run --site=prothom-alo --limit=5 --output=json

# Scrape 3 articles from Ittefaq with verbose logging
python main.py --run --site=ittefaq --limit=3 --output=csv --verbose

# Get ALL available articles from today
python main.py --run --site=prothom-alo --limit=0 --output=json

# Short form commands
python main.py -r -s prothom-alo -l 5 -o json -v
```

### Python API Usage

```python
from bangla_news_scraper import ProthomAloScraper, save_to_json

# Initialize scraper
scraper = ProthomAloScraper(delay=1.0)

# Scrape articles
articles = scraper.scrape_articles(limit=5)

# Save to file
save_to_json(articles, 'prothom-alo', './output')

# Work with individual articles
for article in articles:
    print(f"Title: {article.title}")
    print(f"Author: {article.author}")
    print(f"Content preview: {article.get_content_preview(100)}")
    print(f"Valid: {article.is_valid()}")
```

## üìã Command Line Options

| Option | Short | Default | Description |
|--------|-------|---------|-------------|
| `--run` | `-r` | - | Run the scraper (required) |
| `--site` | `-s` | `prothom-alo` | News site (`prothom-alo`, `ittefaq`) |
| `--limit` | `-l` | `5` | Number of articles (0 = all available) |
| `--output` | `-o` | `json` | Output format (`json`, `csv`) |
| `--output-dir` | `-d` | `output` | Output directory |
| `--delay` | - | `1.0` | Delay between requests (seconds) |
| `--verbose` | `-v` | - | Enable verbose logging |
| `--no-cache` | - | - | Disable caching |
| `--clear-cache` | - | - | Clear cache before running |
| `--cache-stats` | - | - | Show cache statistics |

## üìÅ Project Structure

```
BD-Newspaper-Scraper/
‚îú‚îÄ‚îÄ src/bangla_news_scraper/          # Main package
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                   # Package exports
‚îÇ   ‚îú‚îÄ‚îÄ cli.py                        # Enhanced CLI interface
‚îÇ   ‚îú‚îÄ‚îÄ config.py                     # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ models.py                     # Data models & validation
‚îÇ   ‚îú‚îÄ‚îÄ exceptions.py                 # Custom exceptions
‚îÇ   ‚îú‚îÄ‚îÄ scrapers/                     # Scraper implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py                   # Abstract base scraper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prothom_alo.py           # Prothom Alo implementation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ittefaq.py               # Ittefaq implementation
‚îÇ   ‚îî‚îÄ‚îÄ utils/                        # Utility modules
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py              # Logging setup
‚îÇ       ‚îú‚îÄ‚îÄ output.py                # Output utilities
‚îÇ       ‚îî‚îÄ‚îÄ cache.py                 # Caching system
‚îú‚îÄ‚îÄ output/                          # Output directory (organized)
‚îÇ   ‚îú‚îÄ‚îÄ json/                        # JSON output files
‚îÇ   ‚îî‚îÄ‚îÄ csv/                         # CSV output files
‚îú‚îÄ‚îÄ tests/                           # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py                  # Test configuration
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py               # Model tests
‚îÇ   ‚îî‚îÄ‚îÄ test_cache.py                # Cache tests
‚îú‚îÄ‚îÄ main.py                          # Entry point script
‚îú‚îÄ‚îÄ setup.py                         # Package setup
‚îú‚îÄ‚îÄ requirements.txt                 # Dependencies
‚îú‚îÄ‚îÄ pytest.ini                      # Test configuration
‚îú‚îÄ‚îÄ .gitignore                       # Git ignore rules
‚îî‚îÄ‚îÄ README.md                       # This file
```

## üîß Advanced Usage

### Cache Management

```bash
# Show cache statistics
python main.py --cache-stats

# Clear all cached articles
python main.py --clear-cache

# Run without caching
python main.py -r -s prothom-alo -l 5 --no-cache
```

## üìÇ Output Organization

All output files are automatically organized in the `output/` directory:

```
output/
‚îú‚îÄ‚îÄ json/                           # JSON format outputs
‚îÇ   ‚îú‚îÄ‚îÄ 2025-09-14_prothom-alo.json
‚îÇ   ‚îî‚îÄ‚îÄ 2025-09-14_ittefaq.json
‚îî‚îÄ‚îÄ csv/                            # CSV format outputs
    ‚îú‚îÄ‚îÄ 2025-09-14_prothom-alo.csv
    ‚îî‚îÄ‚îÄ 2025-09-14_ittefaq.csv
```

**Benefits of organized output:**
- üìÅ Clean separation by format type
- üóìÔ∏è Automatic timestamping with site names
- üîç Easy to locate specific outputs
- üö´ Included in .gitignore to avoid accidental commits

### Custom Configuration

```python
from bangla_news_scraper.config import Config

# Modify default settings
Config.DEFAULT_DELAY = 2.0
Config.CACHE_ENABLED = False
Config.LOG_LEVEL = 'DEBUG'

# Get site configuration
site_config = Config.get_site_config('prothom-alo')
print(site_config['base_url'])
```

### Error Handling

```python
from bangla_news_scraper import ProthomAloScraper
from bangla_news_scraper.exceptions import NetworkException, ScraperException

try:
    scraper = ProthomAloScraper()
    articles = scraper.scrape_articles(limit=5)
except NetworkException as e:
    print(f"Network error: {e}")
except ScraperException as e:
    print(f"Scraping error: {e}")
```

## üìä Output Formats

### JSON Format
```json
{
  "articles": [
    {
      "title": "Article Title in Bengali",
      "content": "Full article content...",
      "author": "Author name",
      "date": "2024-01-15T10:30:00+06:00",
      "url": "https://www.prothomalo.com/...",
      "image_url": "https://www.prothomalo.com/path/to/image.jpg",
      "site_name": "Prothom Alo",
      "scraped_at": "2024-01-15T10:30:00"
    }
  ],
  "site_name": "prothom-alo",
  "total_requested": 5,
  "total_found": 8,
  "total_valid": 5,
  "success_rate": 62.5,
  "duration_seconds": 12.34,
  "scraped_at": "2024-01-15T10:30:00"
}
```

### CSV Format
Articles are saved with columns: `title`, `content`, `author`, `date`, `url`, `image_url`, `site_name`, `scraped_at`

## üß™ Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src/bangla_news_scraper

# Run specific test file
pytest tests/test_models.py -v

# Run with verbose output
pytest -v -s
```

## üõ†Ô∏è Development Setup

```bash
# Clone repository
git clone https://github.com/rayatchowdhury/BD-Newspaper-Scraper.git
cd BD-Newspaper-Scraper

# Create virtual environment
python -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate

# Install in development mode
pip install -e .

# Install development dependencies
pip install -r requirements.txt

# Run tests
pytest
```

## üîß Extending the Scraper

### Adding a New Site

1. Create a new scraper class inheriting from `BaseScraper`:

```python
from bangla_news_scraper.scrapers.base import BaseScraper

class NewSiteScraper(BaseScraper):
    @property
    def base_url(self) -> str:
        return "https://newssite.com"
    
    @property 
    def site_name(self) -> str:
        return "News Site"
    
    # Implement required abstract methods
    def get_article_links(self, limit: int = 10) -> List[str]:
        # Implementation here
        pass
    
    def _extract_title(self, soup) -> str:
        # Implementation here
        pass
    
    # ... other required methods
```

2. Add site configuration to `config.py`
3. Update CLI to include the new site option
4. Add tests for the new scraper

## ‚ö° Performance Tips

- Use caching to avoid re-scraping the same articles
- Adjust `--delay` parameter based on website response time
- Use `--limit=0` carefully as it scrapes ALL available articles
- Monitor logs with `--verbose` for debugging
- Clear expired cache periodically with `--clear-cache`

## üö® Rate Limiting & Ethics

- Default delay: 1 second between requests
- Respects robots.txt and website terms of service
- Use responsibly and avoid overloading news websites
- Consider caching to minimize repeated requests
- Always check website policies before scraping

## üìà Performance Benchmarks

- **Memory Usage**: ~50MB for 100 articles
- **Speed**: ~2-3 articles per second (with 1s delay)
- **Cache Hit Rate**: ~85% for repeated runs
- **Success Rate**: ~95% for valid article URLs

## üêõ Troubleshooting

### Common Issues

1. **Import Errors**: Ensure package is installed with `pip install -e .`
2. **Network Timeouts**: Increase delay with `--delay 2.0`
3. **Empty Results**: Try different sections or check website availability
4. **Cache Issues**: Clear cache with `--clear-cache`
5. **Permission Errors**: Check output directory write permissions

### Debug Mode

```bash
# Enable debug logging
bangla-news-scraper -r -s prothom-alo -l 2 --verbose

# Check cache status
bangla-news-scraper --cache-stats

# Test without cache
bangla-news-scraper -r -s prothom-alo -l 1 --no-cache --verbose
```

## üìù Changelog

### Version 1.1.0 (Enhanced Version)
- ‚úÖ Complete architectural refactor
- ‚úÖ Added abstract base scraper class
- ‚úÖ Implemented caching mechanism
- ‚úÖ Added comprehensive data validation
- ‚úÖ Enhanced error handling with custom exceptions
- ‚úÖ Improved CLI with more options
- ‚úÖ Added unit tests and coverage
- ‚úÖ Better logging and configuration management
- ‚úÖ Proper package structure for distribution

### Version 1.0.0 (Original)
- Basic scraping functionality
- JSON and CSV output
- Command-line interface
- Support for Prothom Alo and Ittefaq

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

This project is open source. Feel free to modify and distribute according to your needs.

## üë®‚Äçüíª Author

**Rayat Chowdhury**  
- GitHub: [@rayatchowdhury](https://github.com/rayatchowdhury)
- Version: 1.1.0 (Enhanced)

## üôè Acknowledgments

- Beautiful Soup for HTML parsing
- Click for CLI framework
- Requests for HTTP functionality
- The Bengali journalism community

---

**‚ö†Ô∏è Legal Notice:** Please respect website terms of service and robots.txt. This tool is for educational and research purposes. Always check scraping policies before use.